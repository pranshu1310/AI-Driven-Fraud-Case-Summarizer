{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\n\nfrom datasets import Dataset as HFDataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n)\n\nfrom sklearn.model_selection import train_test_split\n\nfrom .utils import slm_training_data_path, MODELS_DIR, ensure_dir\n\n\ndef prepare_hf_dataset(df: pd.DataFrame, tokenizer, max_input_length=256, max_target_length=80):\n    \"\"\"\n    Convert a pandas dataframe with columns input_text, target_text\n    into a HuggingFace Dataset with tokenized fields.\n    \"\"\"\n    inputs = df[\"input_text\"].tolist()\n    targets = df[\"target_text\"].tolist()\n\n    tokenized_inputs = tokenizer(\n        inputs,\n        max_length=max_input_length,\n        truncation=True,\n        padding=\"max_length\",\n    )\n    tokenized_targets = tokenizer(\n        targets,\n        max_length=max_target_length,\n        truncation=True,\n        padding=\"max_length\",\n    )\n\n    labels = [\n        [(tok if tok != tokenizer.pad_token_id else -100) for tok in label]\n        for label in tokenized_targets[\"input_ids\"]\n    ]\n\n    dataset = {\n        \"input_ids\": tokenized_inputs[\"input_ids\"],\n        \"attention_mask\": tokenized_inputs[\"attention_mask\"],\n        \"labels\": labels,\n    }\n    return HFDataset.from_dict(dataset)\n\n\ndef train_slm_model(\n    model_name: str = \"google/flan-t5-small\",\n    output_subdir: str = \"slm_model\",\n    max_input_length: int = 256,\n    max_target_length: int = 80,\n    num_train_epochs: int = 3,\n):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(\"Using device:\", device)\n\n    df_slm = pd.read_csv(slm_training_data_path())\n\n    train_df, temp_df = train_test_split(df_slm, test_size=0.2, random_state=42)\n    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n\n    print(\"Train/Val/Test sizes:\", len(train_df), len(val_df), len(test_df))\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n\n    train_hf = prepare_hf_dataset(train_df.reset_index(drop=True), tokenizer, max_input_length, max_target_length)\n    val_hf = prepare_hf_dataset(val_df.reset_index(drop=True), tokenizer, max_input_length, max_target_length)\n\n    batch_size = 8\n    args = Seq2SeqTrainingArguments(\n        output_dir=f\"./{output_subdir}\",\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        learning_rate=1e-4,\n        num_train_epochs=num_train_epochs,\n        fp16=torch.cuda.is_available(),\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        gradient_accumulation_steps=2,\n        report_to=\"none\",\n    )\n\n    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\n    trainer = Seq2SeqTrainer(\n        model=model,\n        args=args,\n        train_dataset=train_hf,\n        eval_dataset=val_hf,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n\n    trainer.train()\n\n    # Save under models/slm_model\n    slm_dir = f\"{MODELS_DIR}/{output_subdir}\"\n    ensure_dir(slm_dir)\n    trainer.save_model(slm_dir)\n    tokenizer.save_pretrained(slm_dir)\n\n    print(f\"Saved SLM model + tokenizer to {slm_dir}\")\n    return slm_dir, test_df\n\n\nif __name__ == \"__main__\":\n    train_slm_model()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}