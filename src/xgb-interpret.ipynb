{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport shap\nfrom typing import Tuple\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, classification_report\nfrom xgboost import XGBClassifier\n\nfrom .feature_engineering import CANDIDATE_FEATURES\nfrom .utils import processed_shap_path, MODELS_DIR, ensure_dir\nimport os\n\n\ndef train_xgb_classifier(df: pd.DataFrame) -> Tuple[XGBClassifier, pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n    \"\"\"\n    Train an XGBoost binary classifier on CANDIDATE_FEATURES vs is_anomaly_label.\n    Returns model + train/val split.\n    \"\"\"\n    df = df.copy()\n    max_rows_for_xgb = min(20000, len(df))\n    xgb_df = df.sample(n=max_rows_for_xgb, random_state=42).copy()\n\n    X = xgb_df[CANDIDATE_FEATURES]\n    y = xgb_df[\"is_anomaly_label\"].astype(int)\n\n    X_train, X_val, y_train, y_val = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n\n    xgbmodel = XGBClassifier(\n        objective=\"binary:logistic\",\n        eval_metric=\"auc\",\n        tree_method=\"hist\",\n        max_depth=6,\n        learning_rate=0.1,\n        n_estimators=200,\n        verbosity=0,\n        random_state=42\n    )\n\n    xgbmodel.fit(\n        X_train, y_train,\n        eval_set=[(X_val, y_val)],\n        verbose=False\n    )\n\n    preds_val = xgbmodel.predict_proba(X_val)[:, 1]\n    auc = roc_auc_score(y_val, preds_val)\n    print(f\"âœ… XGBoost validation AUC: {auc:.4f}\")\n\n    preds_bin = (preds_val >= 0.5).astype(int)\n    print(classification_report(y_val, preds_bin, zero_division=0))\n\n    return xgbmodel, X_train, y_train, X_val, y_val\n\n\ndef compute_shap_for_model(\n    model: XGBClassifier,\n    X_train: pd.DataFrame,\n    X_val: pd.DataFrame,\n    full_df: pd.DataFrame\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Compute SHAP values with a model.predict_proba wrapper and attach local\n    SHAP drivers to full_df (per-row).\n    \"\"\"\n    # Background\n    background = X_train.sample(\n        n=min(2000, len(X_train)),\n        random_state=42\n    ).astype(\"float64\")\n\n    def model_predict_proba(X_input):\n        return model.predict_proba(X_input)[:, 1]\n\n    explainer = shap.Explainer(model_predict_proba, background)\n\n    # Global SHAP on a validation sample\n    sample_X = X_val.sample(n=min(4000, len(X_val)), random_state=42).astype(\"float64\")\n    shap_result = explainer(sample_X)\n    shap_values = shap_result.values  # (n_samples, n_features)\n    print(\"SHAP matrix shape:\", shap_values.shape)\n\n    shap_global = (\n        pd.DataFrame({\n            \"feature\": sample_X.columns,\n            \"mean_abs_shap\": np.abs(shap_values).mean(axis=0)\n        })\n        .sort_values(\"mean_abs_shap\", ascending=False)\n        .reset_index(drop=True)\n    )\n\n    print(\"Top 10 SHAP features:\\n\", shap_global.head(10))\n\n    # Local SHAP for all rows we want to enrich\n    df_for_shap = full_df[CANDIDATE_FEATURES].astype(\"float64\")\n    shap_full = explainer(df_for_shap).values  # (N, n_features)\n\n    def top_local_features(shap_row, n=4):\n        abs_vals = np.abs(shap_row)\n        idx = np.argsort(-abs_vals)[:n]\n        return [CANDIDATE_FEATURES[i] for i in idx]\n\n    full_df = full_df.copy()\n    full_df[\"top_shap_features\"] = [\n        top_local_features(shap_full[i], n=4)\n        for i in range(len(shap_full))\n    ]\n\n    # Optional: a human-readable SHAP summary sentence\n    def row_shap_summary(shap_row):\n        abs_vals = np.abs(shap_row)\n        idx = np.argsort(-abs_vals)[:4]\n        parts = []\n        for i in idx:\n            feat = CANDIDATE_FEATURES[i]\n            val = shap_row[i]\n            sign = \"+\" if val >= 0 else \"-\"\n            parts.append(f\"{feat} {sign}{abs(val):.3f}\")\n        return \"Top factors: \" + \", \".join(parts) + \".\"\n\n    full_df[\"shap_summary\"] = [\n        row_shap_summary(shap_full[i])\n        for i in range(len(shap_full))\n    ]\n\n    return shap_global, full_df\n\n\ndef run_xgb_and_shap():\n    \"\"\"\n    Orchestrator: load processed data, train XGB, compute SHAP,\n    and save artifacts.\n    \"\"\"\n    ensure_dir(MODELS_DIR)\n    df = pd.read_csv(processed_shap_path())\n\n    model, X_train, y_train, X_val, y_val = train_xgb_classifier(df)\n    shap_global, df_with_shap = compute_shap_for_model(model, X_train, X_val, df)\n\n    # Save model & SHAP outputs\n    model_path = os.path.join(MODELS_DIR, \"xgb_anomaly_model.json\")\n    model.save_model(model_path)\n    print(f\"Saved XGBoost model to {model_path}\")\n\n    shap_global_path = os.path.join(MODELS_DIR, \"shap_global_top20.csv\")\n    shap_global.head(20).to_csv(shap_global_path, index=False)\n    print(f\"Saved SHAP global importance to {shap_global_path}\")\n\n    # Save enriched dataframe for SLM\n    from .utils import processed_shap_path as enriched_path\n    df_with_shap.to_csv(enriched_path(), index=False)\n    print(f\"Saved SHAP-enriched dataset to {enriched_path()}, shape {df_with_shap.shape}\")\n\n    return df_with_shap, shap_global\n\n\nif __name__ == \"__main__\":\n    run_xgb_and_shap()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}