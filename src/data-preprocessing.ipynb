{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom typing import Optional, List\n\nfrom .utils import default_raw_path, ensure_dir, DATA_PROCESSED_DIR\n\n\ndef load_raw_transactions(path: Optional[str] = None, nrows: Optional[int] = None) -> pd.DataFrame:\n    \"\"\"\n    Load the raw transactions CSV.\n    \"\"\"\n    if path is None:\n        path = default_raw_path()\n    df = pd.read_csv(path, nrows=nrows)\n    return df\n\n\ndef filter_sample_clients(df: pd.DataFrame, n_clients: int = 50) -> pd.DataFrame:\n    \"\"\"\n    For development: keep data only for a subset of client_ids\n    so we can iterate quickly without training on millions of rows.\n    \"\"\"\n    if \"client_id\" not in df.columns:\n        return df\n\n    sample_clients = df[\"client_id\"].dropna().astype(str).unique()[:n_clients]\n    df_small = df[df[\"client_id\"].astype(str).isin(sample_clients)].copy()\n    return df_small\n\n\ndef basic_clean(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Basic cleaning: parse dates, sort, drop duplicates, remove obviously bad rows.\n    Assumes columns similar to your Kaggle dataset.\n    \"\"\"\n    df = df.copy()\n\n    # date parsing\n    if \"txn_date\" in df.columns:\n        df[\"txn_date\"] = pd.to_datetime(df[\"txn_date\"], errors=\"coerce\")\n\n    # drop duplicates by txn_id if present\n    if \"txn_id\" in df.columns:\n        df = df.drop_duplicates(subset=[\"txn_id\"])\n\n    # sort by client_id + date\n    sort_cols = [c for c in [\"client_id\", \"txn_date\"] if c in df.columns]\n    if sort_cols:\n        df = df.sort_values(sort_cols)\n\n    # drop obvious nonsense, e.g., negative absurd amounts (you can adjust)\n    if \"txn_amount\" in df.columns:\n        df = df[df[\"txn_amount\"].notna()]\n\n    return df\n\n\ndef run_preprocessing(\n    raw_path: Optional[str] = None,\n    nrows: Optional[int] = None,\n    n_clients_sample: Optional[int] = 50\n) -> pd.DataFrame:\n    \"\"\"\n    Orchestrator: load, filter, clean, and save a dev-sized dataset.\n    \"\"\"\n    from .utils import processed_fe_path, ensure_dir\n\n    df = load_raw_transactions(raw_path, nrows=nrows)\n    if n_clients_sample:\n        df = filter_sample_clients(df, n_clients_sample)\n    df = basic_clean(df)\n\n    ensure_dir(DATA_PROCESSED_DIR)\n    df.to_csv(processed_fe_path(), index=False)\n    print(f\"Saved cleaned subset to {processed_fe_path()} with shape {df.shape}\")\n\n    return df\n\n\nif __name__ == \"__main__\":\n    run_preprocessing()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}