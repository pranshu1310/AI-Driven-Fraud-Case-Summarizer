{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom typing import List\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nfrom .shap_enrichment import build_generative_prompt_from_row, parse_shap_summary\nfrom .utils import OUTPUTS_DIR, ensure_dir\n\n\ndef load_slm_model(model_dir: str):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(device)\n    return tokenizer, model, device\n\n\ndef generate_batch(\n    tokenizer,\n    model,\n    device,\n    prompts: List[str],\n    max_length: int = 80,\n    temperature: float = 0.7,\n    top_p: float = 0.9,\n    batch_size: int = 8,\n) -> List[str]:\n    model.eval()\n    outputs = []\n\n    for i in range(0, len(prompts), batch_size):\n        batch = prompts[i:i + batch_size]\n        inputs = tokenizer(\n            batch,\n            return_tensors=\"pt\",\n            truncation=True,\n            padding=True,\n            max_length=256,\n        ).to(device)\n\n        with torch.no_grad():\n            gen = model.generate(\n                **inputs,\n                max_length=max_length,\n                temperature=temperature,\n                top_p=top_p,\n                do_sample=True,\n                num_beams=1,\n                early_stopping=True,\n                repetition_penalty=1.05,\n            )\n\n        decoded = tokenizer.batch_decode(gen, skip_special_tokens=True)\n        outputs.extend(decoded)\n\n    return outputs\n\n\ndef run_inference_on_df(df: pd.DataFrame, model_dir: str) -> pd.DataFrame:\n    \"\"\"\n    Given a dataframe that has at least:\n      - txn_amount, txn_city, txn_country\n      - mins_since_prev_txn, txn_cnt_inlast1440mins\n      - shap_summary\n    build prompts and generate narratives.\n    \"\"\"\n    tokenizer, model, device = load_slm_model(model_dir)\n\n    # Parse shap_summary -> _top_shap_pairs\n    df = df.copy()\n    df[\"_top_shap_pairs\"] = df[\"shap_summary\"].apply(parse_shap_summary)\n\n    prompts = []\n    for _, row in df.iterrows():\n        pairs = row[\"_top_shap_pairs\"]\n        prompts.append(build_generative_prompt_from_row(row, pairs))\n\n    generations = generate_batch(tokenizer, model, device, prompts)\n    df[\"generated_narrative\"] = generations\n\n    ensure_dir(OUTPUTS_DIR)\n    out_path = f\"{OUTPUTS_DIR}/summarized_cases.csv\"\n    df.to_csv(out_path, index=False)\n    print(f\"Saved generated narratives to {out_path}, shape {df.shape}\")\n\n    return df\n\n\nif __name__ == \"__main__\":\n    # Example usage: load SHAP-enriched dataset and generate narratives\n    from .utils import processed_shap_path, MODELS_DIR\n    df_shap = pd.read_csv(processed_shap_path())\n    model_dir = f\"{MODELS_DIR}/slm_model\"   # same as you used in training\n    run_inference_on_df(df_shap.head(100), model_dir)","metadata":{"_uuid":"01e52c37-cc35-42b6-99cf-4146fb06b99b","_cell_guid":"2099ee8d-5d4a-449e-91b7-108bedd3e132","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}